### Qwen3 Decoder vs 原始 Transformer Decoder 核心差异对比

|    **对比维度**     |                 **原始 Transformer Decoder**                 |                    **Qwen3 模型 Decoder**                    |                       **优化效果**                       |
| :-----------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
|   **注意力机制**    |                   MHA：Q/K/V 头数完全一致                    |                  GQA：多 Q 头共享一组 KV 头                  | 1. KV Cache 显存占用减半        2. 推理速度加快                          3. 效果接近 MHA |
|    **位置编码**     | 固定正弦余弦编码：直接加到 Embedding 上，仅支持绝对位置，超长序列泛化差 | 旋转位置编码（RoPE）： 对 Q/K 做旋转（非加性），建模相对位置，适配超长序列 | 1. 超长文本泛化能力强                 2. 相对位置建模更贴合语言逻辑 |
|    **归一化层**     |                          LayerNorm                           |                    RMSNorm：仅计算均方根                     |    1. 训练稳定性提升                      2. 计算速度更快    |
|   **归一化位置**    |                     Post-LN：最后归一化                      |                   Pre-LN：所有层前先归一化                   | 1. 支持 28 层 + 深层堆叠2. 梯度回传更顺畅3. 无需额外初始化技巧 |
| **前馈网络（FFN）** |                           标准 FFN                           | SwiGLU 变体：silu (fc1 (x)) + fc2 (x)，双线性支路 + 激活融合 |   1. 表达能力提升                          2. 训练收敛更快   |
|    **Q/K 优化**     |                           无归一化                           |            QK-Norm：注意力计算前对 Q/K 单独归一化            | 1. 注意力分数分布更稳定2. 避免梯度爆炸3. 长序列注意力更聚焦  |
|    **数据精度**     |                       全 float32 精度                        |         主体 bfloat16 精度，仅归一化层临时转 float32         | 1. 显存占用减半                   2. GPU 算力利用率提升      3. 精度损失可忽略 |
